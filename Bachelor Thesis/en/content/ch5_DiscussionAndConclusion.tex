\chapter{Discussion and Conclusion}
The objective of this paper was to compare the performance of various class imbalance classification models when dealing with imbalanced medical/healthcare datasets. For this, DDAE, MWMOTE, SMOTE, RUSBoost, AdaBoost, cost-sensitive decision tree (csDCT), MetaCost, CAdaMEC, self-paced Ensemble Classifier and Iterative Metric Learning (IML) were utilized to classify some testing samples from various imbalanced medical/healthcare datasets. All the imbalanced datasets came from UCI, KEEL and OpenML with different imbalance ratios (IR). The experiments were divided into two stages. In the first stage, each model was used to deal with each given dataset. In addition, a further two experiments focused on the impact of various imbalance ratios and the size of datasets on the performance of models. The second stage focused on the impact of parameters on the DDAE model and the effectiveness of its different components. 

The results of the experiments carried out for this paper are reported in Chapter 4. This section contains the overall discussion of the results and influencing factors of these models.

DDAE yielded the highest value for recall on almost all imbalanced datasets, not only on medical/healthcare data but also on datasets from other sectors. Since the recall only focuses on minority, a higher recall means that most samples with a class label of 1 can be correctly predicted. In the medical or healthcare sector, successfully detecting as many people in genuine need of medical intervention can be the most critical goal. However, this does not mean that recall is the most significant evaluation metric when the classifier is used on medical datasets. For example, if DDAE is used to test whether a person has infected by Corona Virus, even though all the infected cases can be screened out (meaning recall is 1), a low precision means that, compared with actual patients, several times more healthy people will be incorrectly diagnosed as infected. This is undesirable at this stage, and may even cause social catastrophe. According to the description of PRC, it can be seen that the recall and precision cannot be satisfactory at the same time, so it is critical for a class imbalance classification model to achieve an appropriate F1. Comparing with other algorithms like csDCT, RUSBoost and self-paced Ensemble Classifier, the F1 of DDAE is extremely low on some datasets. This illustrates that DDAE is more sensitive than others, so it is easier to predict a sample with a class label of 0 as 1, which can be considered first because the number of training samples is too small. On several datasets, such as Poker8vs6, DDAE's F1 performs extremely poorly compared to other algorithms but has better recall performance. This dataset contains a total of 1,477 samples, but its IR is as high as 85.882, that is, only 17 positive samples are included in this dataset. According to the description of DDAE in Chapter 3, the whole training set was divided into 79 data blocks to train each base classifier with each block only consisting of 26 samples (13 positive samples and 13 negative samples). Although each data block reaches a balanced state, when the total number of samples is small and the degree of imbalance is high, the sub-training set obtained by resampling is incomplete. This may cause the model to overfit to minority, fail to learn more robust and easy generalization features, and often have worse generalization performance on highly skewed data. Under these circumstances, if AWA in DDAE assign a non-default weight to minority, the model will become more sensitive to the minority class, which will cause a low precision. Comparing to Poker8vs6, the performance of F1 on PH1 (with 11,274 data samples) and optdigits (with 5,620 data samples) is much better. This can also be confirmed from Figure \ref{fig17} and Figure \ref{fig16}, which shows that when the size of dataset is constant, as the IR increases, the F1 and AUCPRC of DDAE on individual dataset shows a significant decrease.

Iterative Metric Learning (IML) is a model that also utilizes Large Margin Nearest Neighbor (LMNN) to improve the data space and uses the kNN classifier as the basic classifier. Unlike DDAE, IML always performs well in F1 and AUCPRC, even though the value of recall is low. It can be observed that the evaluation metrics of IML on different datasets varies, meaning that compared with other models, IML is more sensitive to the data distribution of the dataset. From the results in section 4.2.2, it can be observed that, even the IR climbs up, while the evaluation metrics of IML keep stable.

The performance of SMOTE and MWMOTE are similar to some extent, but it is evident that the value of G-mean, F1 and AUCPRC for SMOTE on some slightly imbalanced datasets, such as WDBC (IR=1.866) and PID (IR=1.684), is better than that of MWMOTE. This can also be observed from Figure \ref{fig17}, which shows that the IR is relatively small, SMOTE can achieve a better performance than MWMOTE. SMOTE will randomly select minority samples to synthesize new samples, regardless of the surrounding samples. This may cause the generation of useless samples if the selected minority sample is far away from the decision boundary or the newly generated samples may be overlapped with the majority samples in the surrounding if the selected minority reside inside the majority area \cite{61}. MWMOTE also applies the synthetic sample generation technique. However, unlike SMOTE, MWMOTE utilizes a clustering procedure to ensure that all the produced samples must be located within the minority region to avoid any false or noisy synthetic samples \cite{62}. MWMOTE also finds a more practical approach to select samples that are difficult to learn, and then an appropriate weight will be assigned to them \cite{62}, which SMOTE does not consider. This can explain why MWMOTE performs better than SMOTE under circumstances where the IR is high. 

RUSBoost, AdaBoost and self-paced Ensemble Classifier are three alternative ensemble learning techniques to DDAE. It should be noted that the performance of RUSBoost and AdaBoost is very similar. Compared with RUSBoost, the self-paced Ensemble Classifier and AdaBoost show a more stable performance. Even though RUSBoost and AdaBoost achieve 0.214 and 0.196  respectively in terms of recall on the Pc3 dataset, the recall of the self-paced Ensemble Classifier is 0.786 for this dataset, which yielding the highest value among all models compared. AdaBoost is a basic implementation of the ensemble learning technique. RUSBoost adds a random undersampling technique on the foundation of boosting. Like DDAE, the self-paced Ensemble Classifier also cuts the majority of the dataset into several bins and uses the resampled balance training set (including majority subset and minority subset) to train each base classifier. The self-paced Ensemble Classifier, RUSBoost and AdaBoost, when compared with DDAE, do not take the weight of both classes into concern, which can also contribute to a lower recall value but higher F1 and AUCPRC values. 

Moreover, the remaining of the algorithms are all implementations of cost-sensitive learning, namely MetaCost, csDCT and CAdaMEC. It can be observed that these models can perform well when the IR of the dataset is relatively low, such as in the Euthyroid Sick, optdigits and PH1 datasets. The performance of MetaCost varies when it deals with different datasets. It is hard to set an appropriate cost matrix for each dataset, as people cannot be experts all the time. If the set cost matrix is not suitable for the predicted dataset and the data is highly skewed, the performance of the classifier can be extremely unsatisfactory, such as on the Poker8vs6 dataset. Considering this, it is better to utilize this kind of classifier when the imbalance ratio is not very high. This was also confirmed through the experiments in section 4.2.2, which showed that as the imbalance ratio increases, the curves of all evaluation metrics of MetaCost and CAdaMEC fluctuate, but still show a downward trend.

% It is clear that many models can maintain a high value in terms of recall and G-mean even though the imbalance ratio is great, which means a large proportion of minorities and majorities can be correctly screened out. If the recall stays almost stable or has a slight decrease as the imbalance ratio increase, it can be observed that the precision of this model must show a downward trend, such as in DDAE, csDCT, MWMOTE, SMOTE. This shows that as the data distribution becomes more and more imbalanced, these models may overfit to minority. Since the F1 and AUCPRC take both recall and precision into account, these two metrics also show a downward trend. IML and MetaCost show less relationship with the changes in imbalance ratio. The former has an almost stable performance, and the latter shows an irregular fluctuation during this process. When considering F1 and AUCPRC, AdaBoost and RUSBoost perform better when the imbalance ratio is higher than 30.

The impact of the size of the dataset on the performance of the model is noticeable. Except for Metacost, which dispalys an irregular fluctuation, the remaining algorithms either continue to perform very well or all the evaluation metrics of the algorithm rise gradually as the number of samples in the dataset increases. For most algorithms, the more samples in the dataset, the more training samples are used to train the model, which can provide a stronger training basis for the model and enable more accurate prediction. Enlarging the scale of the dataset can enhance the completeness of the data, and can also alleviate the over-fitting phenomenon caused by the resampling method, such as in MWMOTE and SMOTE.

From section 4.3.1, it can be seen that the recall and G-mean drop significantly when the DBC component has been removed. The variant DDAE-DBC displays the poorest performance among all other variants and the original model. This phenomenon shows that DBC plays a vital role in the process of identifying minority samples. DBC turns the data distribution of each data block into a nearly balanced state, which leads to the improvement of the minority’s ‘status’ so that the model can classify minority samples correctly. However, this component has also cause the problem of overfitting, since the removal of DBC lead to a significant decrease on recall but increase on precision. DBC is also critical because it provides support for AWA and EL components. Due to the different data distribution, AWA components may be ‘abandoned’ sometimes( which means the default weight pair is assigned to the model), but it does not mean AWA is dispensable. On the PH1 dataset, the recall and G-mean of DDAE-AWA perform less satisfactory than the original model. Considering all these evaluation metrics, the original model should be used under some specific circumstances, for instance, when the misclassifcation of majority samples can be ignored.

Given the importance of the DBC component, the selection of an appropriate number of data blocks is also a critical problem. According to the previous description of DBC in Chapter 3, when the original dataset is divided into a small number of blocks, the data distribution in every single block can still maintain imbalance. Thus, it can be seen that DDAE achieves increased performance when the number of data blocks is close to the imbalance ratio value. Nevertheless, the imbalance ratio is not the only option. Notably, the model can also reach a promising performance when the number of data blocks is 4 and 9 on the Cm1 and 10 and 11 on the Mw1 dataset. This phenomenon illustrates that the original dataset might be divided into a smaller number of blocks instead of the value of IR blocks. From this experiment, it can be seen that the number of base estimators set by ensemble learning algorithms also has an impact on the algorithm performance.

When should the two different classes be pushed away from each other, and when should the same class be pulled closer to each other? The answer to this question may lead to the decision of the relative weight $\omega$ between pull and push term in the DSI component. For instance, when the data instances from these two classes overlap to a high degree, it is best to push them away from each other through setting a considerable value of $\omega$. On the contrary, if an area is occupied by the majority class instances, it may be critical to set a smaller value at $\omega$ in order to bring the minority instances with the same label closer together. 
The adoption of the AWA component is decided by a threshold $\tau$ called unstable ratio. According to the result in this section, the best $\tau$ should be set to 0.2.

In AWA, the cost ratio represents the importance of the costs between the majority and minority classes. To be specific, when the cost ratio is set to 1, the majority class and minority class have an equal ‘status’, as if both of them, are the king of their kingdom. However, from the results presented in this section, all the metrics become stable when the cost ratio is greater than two on both of the datasets in the experiment, showing that the different selection of cost ratio does not affect the performance of the model. This shows the data sources do not restrict the DDAE. The overall results in section 4.1.1 also confirm that even when the datasets are from different fields, the DDAE can deal with them well. This matches the finding stated in study \cite{73}.

In this paper, all the experiments focused on the binary classification. However, some of the algorithms used for comparison are also suitable for multi-class classification, such as AdaBoost, MetaCost, cost-sensitive decision tree. SMOTE, MWMOTE and IML focuses on the feature space. Thus, if these techniques are utilized for multi-class classification, the classifier combined with these techniques should be suitable for multi-class classification.

\begin{table}[]
    %\centering
    % \tiny
    \begin{tabular}{|p{0.18\textwidth}<{\centering}|p{0.25\textwidth}|p{0.3\textwidth}|p{0.18\textwidth}|}
    \hline
    \textbf{Technique}        & \textbf{Pros} & \textbf{Cons} & \textbf{Example} \\ \hline
    \textbf{Oversampling} 
    & Balance the class distribution of original dataset                
    & Depend on the distribution of minority samples;\newline Require a lot of additional computing resources;\newline May cause the problem of overfitting
    & SMOTE\newline MWMOTE\newline DDAE\\ \hline
    \textbf{Undersampling}   
    & Balance the class distribution of original dataset                
    & May cause the information loss and the problem of underfitting                
    &RUSBoost\newline DDAE       \\ \hline
    \textbf{Cost-sensitive Learning}            
    & Take the importance of different class into account                
    & Hard to set an appropriate cost matrix for each dataset            & MetaCost\newline CAdaMEC\newline csDCT\newline DDAE  \\ \hline
    \textbf{Ensemble Learning}            
    & Stable;\newline Make more reliable predictions than the single classifier;\newline Reduce the dispersion of model performance                
    & Absorb not only the advantages but also the disadvantages of used techniques when the final classifier is combined with several techniques       
    & RUSBoost\newline AdaBoost\newline self-paced Ensemble Classifier\newline DDAE  \\ \hline
    \textbf{Distance Metric Learning}            
    & Result in a stable neighborhood for each sample                
    & Depend on the data distribution in the feature space, when the dataset is highly imbalance with a relatively small size, the performance can be unsatisfactory               
    &IML\newline DDAE        \\ \hline
    \end{tabular}
    \caption{Pros and Cons of different Techniques utilized for dealing with Class Imbalance Problem}
    \label{tab26}
\end{table}

In summary, this paper investigated the performance comparison of several class imbalance classification models using medical/healthcare datasets and also some datasets from other fields.  Table \ref{tab26} shows the characteristics of techniques used for dealing with class imbalance problem. Since algorithms have various characteristics, it is important to choose the appropriate algorithm depending on the actual situation. For instance, for the algorithms used for stocks prediction, people should pay more attention to their predictions. However, in medical diagnosis or earthquake prediction sector, the recall for the algorithms is more significant. The performance of algorithms cannot be judged by the individual evaluation metric, the actual request and situation should also be taken into account.
% The algorithm that uses undersampling inevitably discards most of the majority samples when the imbalance ratio is high, causing information loss, while the algorithms that uses the oversampling method generates huge amounts of data when the amount of data is large and the imbalance ratio is high, which requires a lot of additional computing resources. Oversampling, such as SMOTE and MWMOTE, is based on the distribution of the minority class samples. When the “quality” of the minority class is poor, this method may be worse than direct training. For cost-sensitive learning methods, such as MetaCost, csDCT and CAdaMEC, it is hard to set an appropriate cost matrix for each dataset as people cannot be experts all the time. If the set cost matrix is not suitable for the to be predicted dataset, the performance of the classifier can be unsatisfactory. Therefore, it is better to utilize this kind of classifier when the imbalance ratio is not very high. Considering ensemble learning methods, its performance can be satisfactory most of the time, but this study identified that due to the ensemble process the final classifier can absorb not only the advantages but also the disadvantages of used techniques. 
In addition, the number of base estimators set by ensemble learning classifiers can also make an impact on their performance. This has already been researched with regard to DDAE, but more detail surrounding the impact of the number of base estimators on other ensemble classifiers remains an area for the future research.
