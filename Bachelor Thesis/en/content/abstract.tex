\begin{abstract}
Recently, machine learning has become a popular topic in the computer field and classification is one of the most prevalent directions in machine learning. There are many standard and commonly used classification methods, such as Na{\"i}ve Bayes, k-nearest neighbor (kNN), support vector machine and decision tree. Most classification algorithms assume that the dataset is balanced distributed, but data obtained from real life can hardly meet this prerequisite. In the real world, the impact of data imbalance in many domains is pervasive, such as in fraud detection, risk management, text classification and medical diagnosis. Especially in the medical or healthcare sector, the cost of missing a patient is much more expensive than that of classifying a healthy person as a patient. If this problem is ignored, it will not be easy to guarantee the accuracy of the classification task.

Due to the prevalence of class imbalance problem in medical or healthcare sector, in order to have a comprehensive and deep understanding of the imbalance classification algorithms, in this paper, a comparison of the binary classification performance of several classical and state-of-the-art class imbalance classification algorithms was carried out on several datasets. The data used for the experiments was taken from UCI, KEEL and OpenML which included not only the data from the medical/healthcare sector but also some other widely used datasets.

The algorithms used for the comparison included DDAE, MWMOTE, SMOTE, RUSBoost, AdaBoost, cost-sensitive decision tree (csDCT), self-paced Ensemble Classifier, MetaCost, CAdaMEC and Iterative Metric Learning (IML), most of which come from three main groups of class imbalance classification, namely sampling, cost-sensitive learning and ensemble learning. The experiments mainly focused on an overall comparison, the impact of imbalance ratio and the size of the given dataset on the performance of the above-mentioned algorithms. Mainly DDAE is researched, including the effectiveness of each component and the impact of parameters upon it. Sampling methods are not suitable all the time since they need to consider the neighborhoods based on distance. However, some classifiers can be improved after the balance of class distribution. Cost-sensitive learning models should be utilized when the dataset is less in imbalance, because it is difficult to set an appropriate cost matrix for the specific dataset, which can cause the fluctuations of their performances. Ensemble learning techniques perform better as they approach the problem from many angles, but they absorb not only the advantages but also the disadvantages of the techniques used. In addition, according to the results, the Data Block Construction (DBC) was the most important component in DDAE. Also, its parameter tuning process showed as the most appropriate parameter for the model, especially in the experiment on the number of data blocks of DDAE, which illustrates the impact of the number of base learners on the performance of ensemble learning algorithms.

\end{abstract}

